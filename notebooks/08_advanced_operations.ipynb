{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Pandas Operations\n",
        "\n",
        "<!--\n",
        "Author: RSK World\n",
        "Website: https://rskworld.in\n",
        "Email: help@rskworld.in\n",
        "Phone: +91 93305 39277\n",
        "Description: Advanced operations including pivoting, time series, and more\n",
        "-->\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook covers advanced Pandas operations including pivot tables, time series operations, and other advanced features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pivot Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample data for pivot table\n",
        "df = pd.DataFrame({\n",
        "    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03', '2024-01-03'],\n",
        "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Region': ['North', 'North', 'South', 'South', 'North', 'South'],\n",
        "    'Sales': [100, 150, 120, 180, 110, 160],\n",
        "    'Quantity': [10, 15, 12, 18, 11, 16]\n",
        "})\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# Create pivot table\n",
        "pivot = pd.pivot_table(df, values='Sales', index='Date', columns='Product', aggfunc='sum')\n",
        "print(\"\\n=== Pivot Table ===\")\n",
        "print(pivot)\n",
        "\n",
        "# Pivot table with multiple aggregations\n",
        "pivot_multi = pd.pivot_table(df, values=['Sales', 'Quantity'], index='Date', columns='Product', aggfunc={'Sales': 'sum', 'Quantity': 'mean'})\n",
        "print(\"\\n=== Pivot Table with Multiple Aggregations ===\")\n",
        "print(pivot_multi)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create time series data\n",
        "dates = pd.date_range('2024-01-01', periods=30, freq='D')\n",
        "ts_df = pd.DataFrame({\n",
        "    'Date': dates,\n",
        "    'Sales': np.random.randint(100, 500, 30),\n",
        "    'Customers': np.random.randint(10, 100, 30)\n",
        "})\n",
        "\n",
        "ts_df.set_index('Date', inplace=True)\n",
        "print(\"=== Time Series DataFrame ===\")\n",
        "print(ts_df.head(10))\n",
        "\n",
        "# Resample by week\n",
        "weekly = ts_df.resample('W').sum()\n",
        "print(\"\\n=== Weekly Aggregation ===\")\n",
        "print(weekly)\n",
        "\n",
        "# Rolling window\n",
        "ts_df['Sales_7day_avg'] = ts_df['Sales'].rolling(window=7).mean()\n",
        "print(\"\\n=== 7-Day Rolling Average ===\")\n",
        "print(ts_df[['Sales', 'Sales_7day_avg']].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Tabulation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample data\n",
        "df_cross = pd.DataFrame({\n",
        "    'Department': ['IT', 'IT', 'HR', 'HR', 'IT', 'Finance', 'IT', 'HR'],\n",
        "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'New York', 'Berlin', 'London', 'Tokyo']\n",
        "})\n",
        "\n",
        "# Cross tabulation\n",
        "crosstab = pd.crosstab(df_cross['Department'], df_cross['City'])\n",
        "print(\"=== Cross Tabulation ===\")\n",
        "print(crosstab)\n",
        "\n",
        "# Cross tab with margins\n",
        "crosstab_margins = pd.crosstab(df_cross['Department'], df_cross['City'], margins=True)\n",
        "print(\"\\n=== Cross Tabulation with Margins ===\")\n",
        "print(crosstab_margins)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Index Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create DataFrame with MultiIndex\n",
        "arrays = [['A', 'A', 'B', 'B'], ['one', 'two', 'one', 'two']]\n",
        "index = pd.MultiIndex.from_arrays(arrays, names=('first', 'second'))\n",
        "df_multi = pd.DataFrame({'value': [1, 2, 3, 4]}, index=index)\n",
        "\n",
        "print(\"=== MultiIndex DataFrame ===\")\n",
        "print(df_multi)\n",
        "\n",
        "# Accessing MultiIndex levels\n",
        "print(\"\\n=== Accessing by first level ===\")\n",
        "print(df_multi.loc['A'])\n",
        "\n",
        "print(\"\\n=== Accessing by both levels ===\")\n",
        "print(df_multi.loc[('A', 'one')])\n",
        "\n",
        "# Stack and Unstack example\n",
        "df_stack_example = pd.DataFrame({\n",
        "    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02'],\n",
        "    'Product': ['A', 'B', 'A', 'B'],\n",
        "    'Sales': [100, 150, 120, 180]\n",
        "})\n",
        "df_pivot_for_stack = df_stack_example.pivot(index='Date', columns='Product', values='Sales')\n",
        "df_pivot_reset = df_pivot_for_stack.reset_index()\n",
        "df_stacked = df_pivot_reset.set_index(['Date', 'Product']).stack()\n",
        "print(\"\\n=== Stacked DataFrame ===\")\n",
        "print(df_stacked)\n",
        "\n",
        "# Unstack\n",
        "df_unstacked = df_stacked.unstack()\n",
        "print(\"\\n=== Unstacked DataFrame ===\")\n",
        "print(df_unstacked)\n",
        "\n",
        "# Cross-section xs() for MultiIndex\n",
        "df_multi_level = pd.DataFrame({\n",
        "    'value': range(12)\n",
        "}, index=pd.MultiIndex.from_product([['A', 'B', 'C'], ['x', 'y'], ['1', '2']], \n",
        "                                     names=['level1', 'level2', 'level3']))\n",
        "print(\"\\n=== MultiIndex with 3 levels ===\")\n",
        "print(df_multi_level)\n",
        "print(\"\\n=== Cross-section at level1='A' ===\")\n",
        "print(df_multi_level.xs('A', level='level1'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Functions and Expanding Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample time series data\n",
        "ts_data = pd.DataFrame({\n",
        "    'Date': pd.date_range('2024-01-01', periods=20, freq='D'),\n",
        "    'Sales': np.random.randint(100, 500, 20),\n",
        "    'Revenue': np.random.randint(1000, 5000, 20)\n",
        "})\n",
        "ts_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Rolling window functions\n",
        "ts_data['Rolling_Mean_3'] = ts_data['Sales'].rolling(window=3).mean()\n",
        "ts_data['Rolling_Std_3'] = ts_data['Sales'].rolling(window=3).std()\n",
        "ts_data['Rolling_Max_5'] = ts_data['Sales'].rolling(window=5).max()\n",
        "\n",
        "# Expanding window\n",
        "ts_data['Expanding_Mean'] = ts_data['Sales'].expanding().mean()\n",
        "ts_data['Expanding_Sum'] = ts_data['Sales'].expanding().sum()\n",
        "\n",
        "# Exponentially weighted moving average\n",
        "ts_data['EWM'] = ts_data['Sales'].ewm(span=5).mean()\n",
        "\n",
        "print(\"=== Window Functions ===\")\n",
        "print(ts_data[['Sales', 'Rolling_Mean_3', 'Expanding_Mean', 'EWM']].head(10))\n",
        "\n",
        "# Rolling window with custom functions\n",
        "def custom_roll_func(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "ts_data['Rolling_Range'] = ts_data['Sales'].rolling(window=5).apply(custom_roll_func)\n",
        "print(\"\\n=== Custom Rolling Function ===\")\n",
        "print(ts_data[['Sales', 'Rolling_Range']].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create DataFrame with categorical data\n",
        "df_cat = pd.DataFrame({\n",
        "    'Product': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
        "    'Category': ['Electronics', 'Clothing', 'Food', 'Electronics', 'Clothing', 'Food'],\n",
        "    'Price': [100, 50, 20, 120, 55, 25]\n",
        "})\n",
        "\n",
        "# Convert to categorical with ordered categories\n",
        "df_cat['Category'] = pd.Categorical(df_cat['Category'], \n",
        "                                     categories=['Food', 'Clothing', 'Electronics'],\n",
        "                                     ordered=True)\n",
        "\n",
        "print(\"=== Categorical DataFrame ===\")\n",
        "print(df_cat)\n",
        "print(\"\\nCategory dtype:\", df_cat['Category'].dtype)\n",
        "print(\"Categories:\", df_cat['Category'].cat.categories)\n",
        "\n",
        "# Memory usage comparison\n",
        "df_cat['Category_str'] = df_cat['Category'].astype(str)\n",
        "print(\"\\n=== Memory Usage Comparison ===\")\n",
        "print(f\"Categorical: {df_cat['Category'].memory_usage(deep=True)} bytes\")\n",
        "print(f\"String: {df_cat['Category_str'].memory_usage(deep=True)} bytes\")\n",
        "\n",
        "# Categorical operations\n",
        "print(\"\\n=== Value Counts ===\")\n",
        "print(df_cat['Category'].value_counts())\n",
        "\n",
        "# Rename categories\n",
        "df_cat['Category'].cat.rename_categories({'Food': 'Grocery'}, inplace=True)\n",
        "print(\"\\n=== After Renaming Categories ===\")\n",
        "print(df_cat['Category'].cat.categories)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced String Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample DataFrame\n",
        "df_str = pd.DataFrame({\n",
        "    'Name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'David Wilson'],\n",
        "    'Email': ['alice.smith@email.com', 'bob.j@email.com', 'charlie@email.com', 'd.wilson@email.com'],\n",
        "    'Phone': ['123-456-7890', '(987) 654-3210', '555.123.4567', '111-222-3333']\n",
        "})\n",
        "\n",
        "print(\"=== Original DataFrame ===\")\n",
        "print(df_str)\n",
        "\n",
        "# Advanced string operations\n",
        "df_str['First_Name'] = df_str['Name'].str.split().str[0]\n",
        "df_str['Last_Name'] = df_str['Name'].str.split().str[-1]\n",
        "df_str['Email_Domain'] = df_str['Email'].str.extract(r'@([^.]+)')\n",
        "df_str['Phone_Cleaned'] = df_str['Phone'].str.replace(r'[^\\d]', '', regex=True)\n",
        "\n",
        "# Pattern matching\n",
        "df_str['Has_Middle'] = df_str['Name'].str.contains(r'\\s+\\w+\\s+', regex=True)\n",
        "\n",
        "# String padding\n",
        "df_str['Phone_Formatted'] = df_str['Phone_Cleaned'].str.pad(width=10, side='left', fillchar='0')\n",
        "\n",
        "print(\"\\n=== After String Operations ===\")\n",
        "print(df_str[['Name', 'First_Name', 'Last_Name', 'Email_Domain', 'Phone_Cleaned', 'Has_Middle']])\n",
        "\n",
        "# String concatenation\n",
        "df_str['Full_Email'] = df_str['First_Name'].str.lower() + '.' + df_str['Last_Name'].str.lower() + '@email.com'\n",
        "print(\"\\n=== Generated Emails ===\")\n",
        "print(df_str[['Name', 'Full_Email']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Large Datasets (Chunking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Example: Reading large CSV files in chunks\n",
        "# This is a demonstration - in practice, use this for files that don't fit in memory\n",
        "\n",
        "print(\"=== Reading CSV in Chunks ===\")\n",
        "print(\"Example code for processing large files:\")\n",
        "\n",
        "chunk_list = []\n",
        "# Uncomment the following to process a large file in chunks:\n",
        "# for chunk in pd.read_csv('large_file.csv', chunksize=1000):\n",
        "#     # Process each chunk\n",
        "#     chunk_filtered = chunk[chunk['column'] > 100]\n",
        "#     chunk_list.append(chunk_filtered)\n",
        "# \n",
        "# # Combine all chunks\n",
        "# df_combined = pd.concat(chunk_list, ignore_index=True)\n",
        "\n",
        "# Alternative: Using iterator parameter\n",
        "# for chunk in pd.read_csv('large_file.csv', iterator=True, chunksize=1000):\n",
        "#     process(chunk)\n",
        "\n",
        "print(\"For large datasets, process data in chunks to manage memory efficiently.\")\n",
        "\n",
        "# Memory optimization tips\n",
        "df_sample = pd.DataFrame({\n",
        "    'int_col': [1, 2, 3, 4, 5] * 1000,\n",
        "    'float_col': [1.1, 2.2, 3.3, 4.4, 5.5] * 1000,\n",
        "    'str_col': ['a', 'b', 'c', 'd', 'e'] * 1000\n",
        "})\n",
        "\n",
        "print(\"\\n=== Memory Usage ===\")\n",
        "print(f\"Original memory: {df_sample.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "\n",
        "# Optimize data types\n",
        "df_optimized = df_sample.copy()\n",
        "df_optimized['int_col'] = df_optimized['int_col'].astype('int8')\n",
        "df_optimized['float_col'] = df_optimized['float_col'].astype('float32')\n",
        "df_optimized['str_col'] = df_optimized['str_col'].astype('category')\n",
        "\n",
        "print(f\"Optimized memory: {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "print(f\"Memory saved: {(1 - df_optimized.memory_usage(deep=True).sum() / df_sample.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Validation and Error Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create DataFrame with potential issues\n",
        "df_validate = pd.DataFrame({\n",
        "    'Age': [25, 30, 150, 28, -5],  # Invalid ages\n",
        "    'Salary': [50000, -10000, 70000, 0, 65000],  # Invalid salaries\n",
        "    'Email': ['valid@email.com', 'invalid', 'test@email.com', 'another@email.com', 'bad.email'],\n",
        "    'Name': ['Alice', 'Bob', None, 'David', 'Eve']\n",
        "})\n",
        "\n",
        "print(\"=== Original DataFrame with Issues ===\")\n",
        "print(df_validate)\n",
        "\n",
        "# Data validation\n",
        "# Check for invalid ages (0-120)\n",
        "invalid_ages = df_validate[(df_validate['Age'] < 0) | (df_validate['Age'] > 120)]\n",
        "print(\"\\n=== Invalid Ages ===\")\n",
        "print(invalid_ages[['Name', 'Age']])\n",
        "\n",
        "# Check for invalid salaries (positive values)\n",
        "invalid_salaries = df_validate[df_validate['Salary'] <= 0]\n",
        "print(\"\\n=== Invalid Salaries ===\")\n",
        "print(invalid_salaries[['Name', 'Salary']])\n",
        "\n",
        "# Validate email format using regex\n",
        "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "invalid_emails = df_validate[~df_validate['Email'].str.match(email_pattern, na=False)]\n",
        "print(\"\\n=== Invalid Emails ===\")\n",
        "print(invalid_emails[['Name', 'Email']])\n",
        "\n",
        "# Handle errors gracefully when converting types\n",
        "df_safe = df_validate.copy()\n",
        "try:\n",
        "    df_safe['Age'] = pd.to_numeric(df_safe['Age'], errors='coerce')\n",
        "except Exception as e:\n",
        "    print(f\"Error converting Age: {e}\")\n",
        "\n",
        "print(\"\\n=== After Safe Type Conversion ===\")\n",
        "print(df_safe)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Indexing and Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create DataFrame with custom index\n",
        "df_idx = pd.DataFrame({\n",
        "    'Product': ['A', 'B', 'C', 'D', 'E'],\n",
        "    'Sales': [100, 150, 200, 120, 180],\n",
        "    'Region': ['North', 'South', 'North', 'South', 'North']\n",
        "}, index=pd.date_range('2024-01-01', periods=5, freq='D'))\n",
        "\n",
        "print(\"=== DataFrame with Date Index ===\")\n",
        "print(df_idx)\n",
        "\n",
        "# Index operations\n",
        "print(\"\\n=== Index Operations ===\")\n",
        "print(f\"Index type: {type(df_idx.index)}\")\n",
        "print(f\"Index values: {df_idx.index.values}\")\n",
        "print(f\"Index name: {df_idx.index.name}\")\n",
        "\n",
        "# Set index name\n",
        "df_idx.index.name = 'Date'\n",
        "print(\"\\n=== After Setting Index Name ===\")\n",
        "print(df_idx)\n",
        "\n",
        "# Reset index\n",
        "df_reset = df_idx.reset_index()\n",
        "print(\"\\n=== After Resetting Index ===\")\n",
        "print(df_reset)\n",
        "\n",
        "# Set new index\n",
        "df_new_idx = df_reset.set_index('Product')\n",
        "print(\"\\n=== With Product as Index ===\")\n",
        "print(df_new_idx)\n",
        "\n",
        "# Select by index value\n",
        "print(\"\\n=== Select by Index Value ===\")\n",
        "print(df_new_idx.loc['A'])\n",
        "\n",
        "# Boolean indexing with query\n",
        "print(\"\\n=== Using query() ===\")\n",
        "print(df_idx.query('Sales > 150'))\n",
        "\n",
        "# Using at and iat for faster single value access\n",
        "print(\"\\n=== Fast Single Value Access ===\")\n",
        "print(f\"Value at [0, 'Sales']: {df_idx.at[df_idx.index[0], 'Sales']}\")\n",
        "print(f\"Value at [0, 1]: {df_idx.iat[0, 1]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export and Import Different Formats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample DataFrame\n",
        "df_export = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [25, 30, 35],\n",
        "    'Salary': [50000, 60000, 70000]\n",
        "})\n",
        "\n",
        "print(\"=== Sample DataFrame ===\")\n",
        "print(df_export)\n",
        "\n",
        "# Export to different formats\n",
        "print(\"\\n=== Export Options ===\")\n",
        "\n",
        "# CSV\n",
        "# df_export.to_csv('output.csv', index=False)\n",
        "\n",
        "# Excel (requires openpyxl)\n",
        "# df_export.to_excel('output.xlsx', index=False, sheet_name='Data')\n",
        "\n",
        "# JSON\n",
        "json_str = df_export.to_json(orient='records', indent=2)\n",
        "print(\"JSON format:\")\n",
        "print(json_str[:200] + \"...\")\n",
        "\n",
        "# HTML\n",
        "html_str = df_export.to_html(index=False)\n",
        "print(\"\\nHTML format (first 200 chars):\")\n",
        "print(html_str[:200] + \"...\")\n",
        "\n",
        "# Parquet (efficient binary format)\n",
        "# df_export.to_parquet('output.parquet')\n",
        "\n",
        "# Reading from different formats\n",
        "print(\"\\n=== Reading Different Formats ===\")\n",
        "print(\"# CSV: pd.read_csv('file.csv')\")\n",
        "print(\"# Excel: pd.read_excel('file.xlsx', sheet_name='Sheet1')\")\n",
        "print(\"# JSON: pd.read_json('file.json')\")\n",
        "print(\"# Parquet: pd.read_parquet('file.parquet')\")\n",
        "print(\"# HTML: pd.read_html('file.html')\")\n",
        "print(\"# SQL: pd.read_sql('SELECT * FROM table', connection)\")\n",
        "\n",
        "# Reading from clipboard\n",
        "print(\"\\n=== Clipboard Operations ===\")\n",
        "print(\"# Copy to clipboard: df.to_clipboard()\")\n",
        "print(\"# Read from clipboard: df = pd.read_clipboard()\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Optimization Tips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "import time\n",
        "\n",
        "# Create large DataFrame for performance testing\n",
        "large_df = pd.DataFrame({\n",
        "    'A': np.random.randint(1, 100, 10000),\n",
        "    'B': np.random.randn(10000),\n",
        "    'C': np.random.choice(['X', 'Y', 'Z'], 10000)\n",
        "})\n",
        "\n",
        "print(\"=== Performance Optimization Tips ===\")\n",
        "\n",
        "# 1. Use vectorized operations instead of loops\n",
        "start = time.time()\n",
        "result1 = large_df['A'] * 2  # Vectorized\n",
        "time1 = time.time() - start\n",
        "\n",
        "print(f\"\\n1. Vectorized operation: {time1*1000:.4f} ms\")\n",
        "\n",
        "# 2. Use .loc instead of chained indexing\n",
        "start = time.time()\n",
        "# Good: large_df.loc[large_df['A'] > 50, 'B'] = 999\n",
        "time2 = time.time() - start\n",
        "\n",
        "print(f\"2. Using .loc for assignment: {time2*1000:.4f} ms\")\n",
        "\n",
        "# 3. Use appropriate data types\n",
        "print(\"\\n3. Data Type Optimization:\")\n",
        "print(f\"   Original C column type: {large_df['C'].dtype}\")\n",
        "large_df['C'] = large_df['C'].astype('category')\n",
        "print(f\"   After converting to category: {large_df['C'].dtype}\")\n",
        "\n",
        "# 4. Use query() for filtering (often faster for complex conditions)\n",
        "start = time.time()\n",
        "filtered = large_df.query('A > 50 and B > 0')\n",
        "time3 = time.time() - start\n",
        "print(f\"\\n4. Using query(): {time3*1000:.4f} ms\")\n",
        "\n",
        "# 5. Avoid using apply() when vectorized operations exist\n",
        "start = time.time()\n",
        "result2 = large_df['A'].apply(lambda x: x * 2)  # Slower\n",
        "time4 = time.time() - start\n",
        "print(f\"5. Using apply() (slower): {time4*1000:.4f} ms\")\n",
        "\n",
        "# 6. Use copy() only when necessary\n",
        "print(\"\\n6. Use copy() only when needed:\")\n",
        "df_copy = large_df.copy()  # Only when you need independent copy\n",
        "\n",
        "# 7. Use inplace=True to save memory\n",
        "large_df['D'] = large_df['A'] + large_df['B']\n",
        "# Instead of: large_df = large_df.assign(D=large_df['A'] + large_df['B'])\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(\"- Use vectorized operations\")\n",
        "print(\"- Choose appropriate data types\")\n",
        "print(\"- Use .loc and .iloc properly\")\n",
        "print(\"- Avoid chained indexing\")\n",
        "print(\"- Use query() for complex filters\")\n",
        "print(\"- Minimize use of apply() and loops\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SQL-like Operations with Pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Author: RSK World | Website: https://rskworld.in | Email: help@rskworld.in | Phone: +91 93305 39277\n",
        "\n",
        "# Create sample DataFrames for SQL-like operations\n",
        "df_orders = pd.DataFrame({\n",
        "    'order_id': [1, 2, 3, 4, 5],\n",
        "    'customer_id': [101, 102, 101, 103, 102],\n",
        "    'amount': [100, 200, 150, 300, 250],\n",
        "    'date': pd.date_range('2024-01-01', periods=5, freq='D')\n",
        "})\n",
        "\n",
        "df_customers = pd.DataFrame({\n",
        "    'customer_id': [101, 102, 103],\n",
        "    'name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'city': ['New York', 'London', 'Tokyo']\n",
        "})\n",
        "\n",
        "print(\"=== Orders DataFrame ===\")\n",
        "print(df_orders)\n",
        "print(\"\\n=== Customers DataFrame ===\")\n",
        "print(df_customers)\n",
        "\n",
        "# SQL-like JOIN operations\n",
        "# INNER JOIN\n",
        "df_inner = pd.merge(df_orders, df_customers, on='customer_id', how='inner')\n",
        "print(\"\\n=== INNER JOIN ===\")\n",
        "print(df_inner)\n",
        "\n",
        "# LEFT JOIN\n",
        "df_left = pd.merge(df_orders, df_customers, on='customer_id', how='left')\n",
        "print(\"\\n=== LEFT JOIN ===\")\n",
        "print(df_left)\n",
        "\n",
        "# SQL-like WHERE clause (filtering)\n",
        "df_filtered = df_orders[df_orders['amount'] > 150]\n",
        "print(\"\\n=== WHERE amount > 150 ===\")\n",
        "print(df_filtered)\n",
        "\n",
        "# SQL-like GROUP BY with aggregation\n",
        "df_grouped = df_orders.groupby('customer_id').agg({\n",
        "    'amount': ['sum', 'mean', 'count'],\n",
        "    'order_id': 'count'\n",
        "}).reset_index()\n",
        "df_grouped.columns = ['customer_id', 'total_amount', 'avg_amount', 'order_count', 'total_orders']\n",
        "print(\"\\n=== GROUP BY customer_id ===\")\n",
        "print(df_grouped)\n",
        "\n",
        "# SQL-like ORDER BY (sorting)\n",
        "df_sorted = df_orders.sort_values('amount', ascending=False)\n",
        "print(\"\\n=== ORDER BY amount DESC ===\")\n",
        "print(df_sorted)\n",
        "\n",
        "# SQL-like HAVING (filtering after groupby)\n",
        "df_having = df_orders.groupby('customer_id')['amount'].sum().reset_index()\n",
        "df_having = df_having[df_having['amount'] > 200]\n",
        "print(\"\\n=== HAVING total_amount > 200 ===\")\n",
        "print(df_having)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
